%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Xing at 2017-03-14 01:21:25 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{smote,
	Author = {Chawla},
	Date-Added = {2017-03-14 00:08:37 +0000},
	Date-Modified = {2017-03-14 00:20:57 +0000},
	Title = {SMOTE: Synthetic Minority Over-sampling Technique},
	Year = {2002}}

@article{Terry,
	Author = {Terry, M. T.},
	Date-Added = {2017-03-14 00:07:14 +0000},
	Date-Modified = {2017-03-14 00:08:26 +0000},
	Title = {An Introduction to Recursive Partitioning},
	Year = {2015}}

@book{quinlan,
	Author = {Quinlan},
	Date-Added = {2017-03-13 23:53:57 +0000},
	Date-Modified = {2017-03-13 23:56:22 +0000},
	Journal = {81-106, Kluwer Academic Publishers},
	Publisher = {81-106, Kluwer Academic Publishers},
	Title = {Induction of Decision Trees. Machine Learning 1},
	Year = {1988}}

@article{moro2014data,
	Author = {Moro, S{\'e}rgio and Cortez, Paulo and Rita, Paulo},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Journal = {Decision Support Systems},
	Pages = {22--31},
	Publisher = {Elsevier},
	Title = {A data-driven approach to predict the success of bank telemarketing},
	Volume = {62},
	Year = {2014}}

@book{provost2013data,
	Author = {Provost, Foster and Fawcett, Tom},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Publisher = {" O'Reilly Media, Inc."},
	Title = {Data Science for Business: What you need to know about data mining and data-analytic thinking},
	Year = {2013}}

@article{hornik1989multilayer,
	Author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Journal = {Neural networks},
	Number = {5},
	Pages = {359--366},
	Publisher = {Elsevier},
	Title = {Multilayer feedforward networks are universal approximators},
	Volume = {2},
	Year = {1989}}

@article{nobibon2011optimization,
	Author = {Nobibon, Fabrice Talla and Leus, Roel and Spieksma, Frits CR},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Journal = {European Journal of Operational Research},
	Number = {3},
	Pages = {670--683},
	Publisher = {Elsevier},
	Title = {Optimization models for targeted offers in direct marketing: Exact and heuristic algorithms},
	Volume = {210},
	Year = {2011}}

@article{rust2010rethinking,
	Author = {Rust, Roland T and Moorman, Christine and Bhalla, Gaurav},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Journal = {Harvard business review},
	Number = {1/2},
	Pages = {94--101},
	Title = {Rethinking marketing},
	Volume = {88},
	Year = {2010}}

@book{shiffman2012nature,
	Author = {Shiffman, Daniel and Fry, Shannon and Marsh, Zannah},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Publisher = {D. Shiffman},
	Title = {The nature of code},
	Year = {2012}}

@article{baesens2003using,
	Author = {Baesens, Bart and Setiono, Rudy and Mues, Christophe and Vanthienen, Jan},
	Date-Added = {2017-03-13 21:24:56 +0000},
	Date-Modified = {2017-03-13 21:24:56 +0000},
	Journal = {Management science},
	Number = {3},
	Pages = {312--329},
	Publisher = {INFORMS},
	Title = {Using neural network rule extraction and decision tables for credit-risk evaluation},
	Volume = {49},
	Year = {2003}}

@article{stef2015mice,
	Author = {Stef van Buuren and Karin Groothuis-Oudshoorn},
	Journal = {Journal of Statistical Software},
	Number = {2},
	Pages = {1--67},
	Publisher = {American Statistical Association},
	Title = {MICE: Multivariate Imputation by Chained Equations in R},
	Volume = {10},
	Year = {2015}}

@article{tom2006an,
	Author = {Tom Fawcett},
	Journal = {Pattern Recognition Letters},
	Number = {8},
	Pages = {861--874},
	Publisher = {Pattern Recognition Letters},
	Title = {An introduction to roc analysis},
	Volume = {27},
	Year = {2006}}

@article{david2011performance,
	Author = {David Martens and Jan Vanthienen and Wouter Verbeke and Bart Baesens},
	Journal = {Decision Support Systems},
	Number = {4},
	Pages = {782--793},
	Publisher = {Decision Support Systems},
	Title = {Performance of classification models from a user perspective},
	Volume = {51},
	Year = {2011}}

@article{stef2007multiple,
	Author = {Stef van Buuren},
	Journal = {Statistical Methods in Medical Research},
	Number = {3},
	Pages = {219--242},
	Publisher = {JSTOR},
	Title = {``Multiple imputation of discrete and continuous data by fully conditional specification},
	Volume = {16},
	Year = {2007}}

@book{JL1997analysis,
	Author = {JL Sharfer},
	Publisher = {Chapman \& Hall},
	Title = {Analysis of incomplete multivariate data},
	Year = {1997}}

@book{DB1987multiple,
	Author = {DB Rubin},
	Publisher = {Wiley},
	Title = {Multiple imputation for nonresponse in surveys},
	Year = {1987}}

@article{rubin1996multiple,
	Author = {Rubin DB},
	Journal = {Journal of the American Statistical Association},
	Pages = {473--489},
	Publisher = {JSTOR},
	Title = {Multiple Imputation after 18+ Years},
	Year = {1996}}

@article{sharpe1964capital,
	Author = {Sharpe, William F},
	Journal = {The journal of finance},
	Number = {3},
	Pages = {425--442},
	Publisher = {Wiley Online Library},
	Title = {Capital asset prices: A theory of market equilibrium under conditions of risk},
	Volume = {19},
	Year = {1964}}

@article{MA1987thecalculation,
	Author = {MA Tanner and WH Wong},
	Journal = {J Am Statist Assoc},
	Number = {82},
	Pages = {528--540},
	Publisher = {JSTOR},
	Title = {The calculation of posterior distributions by data augmentation},
	Year = {1987}}

@article{Rachael2014joint,
	Author = {Rachael A Hughes},
	Journal = {BMC Med Res Methodol},
	Number = {28},
	Pages = {528--540},
	Publisher = {BMC Med Res Methodol},
	Title = {Joint modelling rationale for chained equations},
	Volume = {14},
	Year = {2014}}

@book{p1982generalized,
	Author = {P McCullagh and JA Nelder},
	Publisher = {Chapman \& Hall},
	Title = {Generalized Linear Models},
	Year = {1982}}

@book{max2016applied,
	Author = {Max Kuhn and Kjell Johnson},
	Publisher = {Springerl},
	Title = {applied predictive modeling},
	Year = {2016}}

@Article{Breiman1996,
	author="Breiman, Leo",
	title="Bagging predictors",
	journal="Machine Learning",
	year="1996",
	volume="24",
	number="2",
	pages="123--140",
	abstract="Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning 			sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.",
	issn="1573-0565",
	doi="10.1007/BF00058655",
	url="http://dx.doi.org/10.1007/BF00058655"
	}


@Article{Breiman2001,
	author="Breiman, Leo",
	title="Random Forests",
	journal="Machine Learning",
	year="2001",
	volume="45",
	number="1",
	pages="5--32",
	abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on 			the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor 			error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
	issn="1573-0565",
	doi="10.1023/A:1010933404324",
	url="http://dx.doi.org/10.1023/A:1010933404324"
	}

@ARTICLE{4016560, 
	author={R. E. Banfield and L. O. Hall and K. W. Bowyer and W. P. Kegelmeyer}, 
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={A Comparison of Decision Tree Ensemble Creation Techniques}, 
	year={2007}, 
	volume={29}, 
	number={1}, 
	pages={173-180}, 
	keywords={decision trees;learning (artificial intelligence);pattern classification;random processes;statistical analysis;bagging;decision tree classifier;decision tree ensemble creation technique;random forest;randomization-based approach;statistical test;Bagging;Boosting;Classification tree analysis;Decision trees;Performance evaluation;Sampling methods;Statistical 						analysis;Testing;Training data;Classifier ensembles;bagging;boosting;performance evaluation.;random forests;random subspaces;Algorithms;Artificial Intelligence;Decision Support Techniques;Information Storage and Retrieval;Pattern Recognition, Automated}, 
	doi={10.1109/TPAMI.2007.250609}, 
	ISSN={0162-8828}, 
	month={Jan},}

